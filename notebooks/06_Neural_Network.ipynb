{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "A neural network is created which can be used for training and testing on trips data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.11.4)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.30.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (49.1.0.post20200704)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.20.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output=os.path.join(os.getcwd(), \"..\", \"data\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(os.path.join(path_output, \"Features.csv\"))\n",
    "features = features.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Params from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"data\", \"input\", \"params.csv\")).drop(\"0\", axis=1)\n",
    "_test_size = params[params[\"param\"]==\"test_size\"][\"value\"].values[0]\n",
    "_random_state = int(params[params[\"param\"]==\"random_state\"][\"value\"].values[0])\n",
    "_epochs = int(params[params[\"param\"]==\"epochs\"][\"value\"].values[0])\n",
    "_validation_size = params[params[\"param\"]==\"validation_size\"][\"value\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train_scaled, y_train, on, hex_size):\n",
    "    \"\"\"\n",
    "    Train Neural Network Model\n",
    "\n",
    "    Train and save a Neural Network model.\n",
    "    The network has the following properties:\n",
    "        - three hidden layer\n",
    "        - 50 epochs\n",
    "        - activation function is relu\n",
    "        - dimension of input and hidden layer is 36\n",
    "        - dimension of output layer is 1\n",
    "        - dropout is not used\n",
    "    Then evaluate the error metrics by another method.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled (DataFrame):   Scaled X input of train set (matrix)\n",
    "        y_train (Series):             y output to train on (vector)\n",
    "    Returns:\n",
    "        nn_regression_sets (array): true y values and predicted y values for train and validation set\n",
    "    \"\"\"\n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train, random_state=_random_state, test_size=_validation_size)\n",
    "    neural_network = keras.Sequential(\n",
    "        [layers.Dense(36, activation=\"relu\", input_shape=[X_train_scaled.shape[1]], kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dropout(0.2),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"softmax\"),\n",
    "         # layers.Dense(36, activation=\"softmax\"),\n",
    "         # layers.Dropout(0.2),\n",
    "         layers.Dense(1)])\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "    neural_network.compile(loss=\"mse\",\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=[\"mae\", \"mse\"])\n",
    "    epochs = _epochs\n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    history = neural_network.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
    "    neural_network.save(os.path.join(path_output, \"models\", \"NN_Regression_Model_\"+on+\"_\"+hex_size))\n",
    "    y_prediction_train = neural_network.predict(X_train)\n",
    "    y_prediction_val = neural_network.predict(X_val)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_prediction_train)\n",
    "    print(\"R2 (Train):\",r2_train)\n",
    "    r2_val = r2_score(y_val, y_prediction_val)\n",
    "    print(\"R2 (Validation):\",r2_val)\n",
    "    \n",
    "    plot_train_loss(history, on=on, hex_size=hex_size)\n",
    "    nn_regression_sets = [y_train, y_val, y_prediction_train, y_prediction_val]\n",
    "    return nn_regression_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss visualization by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss(history, on, hex_size):\n",
    "    \"\"\"\n",
    "    Plot the train and validation loss of Neural Network.\n",
    "\n",
    "    Args:\n",
    "        history (Object): History of loss during training of neural network\n",
    "        on (str): time resolution to train on\n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Plot training and visualization loss...\")\n",
    "    # Plotting the training and validation loss\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    fig, ax = plt.subplots(figsize=(16, 8), dpi=300)\n",
    "    ax.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    ax.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    ax.set_title(\"Training and validation loss \"+on+\"_\"+hex_size, fontsize=18)\n",
    "    ax.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    fig.savefig(os.path.join(path_output, \"NN_error_per_epoch_\"+on+\"_\"+hex_size+\".png\"))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_NN(on=\"24_demand\", hex_size=\"hexa_big\"):\n",
    "    \"\"\"\n",
    "    Split the data in train and test set by 0.3 test set. \n",
    "    Then Scale the data and do a PCA. \n",
    "    Last train the NN on the chosen time resolution\n",
    "    \n",
    "    Args:\n",
    "        on (str): time resolution to train on\n",
    "        \n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Time Resolution is\", on)\n",
    "    #print(\"Split Data with random state\", _random_state, \"and test size\", str(_test_size)+\"...\")\n",
    "    # features_X = features.drop([\"24_sum\", \"6_sum\", \"2_sum\", \"1_sum\"], axis=1)\n",
    "    features_X = features.drop([\"24_demand\", \"24_demand_hex_big\", \"24_demand_hex_small\", \"24_agg_time\",\n",
    "                                \"6_demand\", \"6_demand_hex_big\", \"6_demand_hex_small\", \"6_agg_time\",\n",
    "                                \"2_demand\", \"2_demand_hex_big\", \"2_demand_hex_small\", \"2_agg_time\",\n",
    "                                \"1_demand\", \"1_demand_hex_big\", \"1_demand_hex_small\", \"1_agg_time\",\n",
    "                                \"24_available_hex_big\"], axis=1)\n",
    "    features_y = features[on]\n",
    "    \n",
    "    # Spatial Resolution\n",
    "    print(\"Spatial Resolution is\", hex_size)\n",
    "    if hex_size==\"hexa_small\":\n",
    "        features_X = features_X.drop(\"hexa_big\", axis=1)\n",
    "    else:\n",
    "        features_X = features_X.drop(\"hexa_small\", axis=1)\n",
    "    \n",
    "    #Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_X, features_y, random_state=_random_state, test_size=_test_size)\n",
    "\n",
    "    #print(\"Scale\", hex_size, \"Data with Standard Scaler...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"Standard_Scaler_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        standard_scaler = pickle.load(f)\n",
    "    X_train_scaled = standard_scaler.transform(X_train)\n",
    "\n",
    "    #print(\"Do PCA on\", hex_size, \"Data...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"PCA_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        pca = pickle.load(f)\n",
    "    X_train_transformed = pca.transform(X_train_scaled)\n",
    "\n",
    "    print(\"Train\", \"NN_Regression_Model_\"+on+\"_\"+hex_size, \"...\")\n",
    "    nn_regression_sets = train_neural_network(X_train_transformed, y_train.to_numpy(), on=on, hex_size=hex_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Resolution is 24_available_hex_big\n",
      "Spatial Resolution is hexa_big\n",
      "Train NN_Regression_Model_24_available_hex_big_hexa_big ...\n",
      "Epoch 1/20\n",
      "8867/8867 [==============================] - 13s 2ms/step - loss: 20088.3008 - mae: 118.5551 - mse: 20088.3008 - val_loss: 18588.7285 - val_mae: 116.3619 - val_mse: 18588.7285\n",
      "Epoch 2/20\n",
      "8867/8867 [==============================] - 12s 1ms/step - loss: 18175.3242 - mae: 113.0830 - mse: 18175.3242 - val_loss: 17396.5469 - val_mae: 110.0716 - val_mse: 17396.5469\n",
      "Epoch 3/20\n",
      "8867/8867 [==============================] - 12s 1ms/step - loss: 16937.5059 - mae: 107.6971 - mse: 16937.5059 - val_loss: 16750.1719 - val_mae: 106.1913 - val_mse: 16750.1719\n",
      "Epoch 4/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 15995.8574 - mae: 103.9586 - mse: 15995.8574 - val_loss: 15902.2354 - val_mae: 103.2684 - val_mse: 15902.2354\n",
      "Epoch 5/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 15242.7139 - mae: 99.8790 - mse: 15242.7139 - val_loss: 14648.4736 - val_mae: 97.3355 - val_mse: 14648.4736\n",
      "Epoch 6/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 14158.2285 - mae: 94.0854 - mse: 14158.2285 - val_loss: 13087.3613 - val_mae: 90.9679 - val_mse: 13087.3613\n",
      "Epoch 7/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 12876.3467 - mae: 87.6240 - mse: 12876.3467 - val_loss: 11686.9258 - val_mae: 83.9140 - val_mse: 11686.9258\n",
      "Epoch 8/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 11094.3604 - mae: 81.0472 - mse: 11094.3604 - val_loss: 10418.2451 - val_mae: 77.2562 - val_mse: 10418.2451\n",
      "Epoch 9/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 10723.9795 - mae: 75.8465 - mse: 10723.9795 - val_loss: 10267.5732 - val_mae: 75.6611 - val_mse: 10267.5732\n",
      "Epoch 10/20\n",
      "8867/8867 [==============================] - 14s 2ms/step - loss: 9886.9688 - mae: 70.9239 - mse: 9886.9688 - val_loss: 8664.2881 - val_mae: 67.4212 - val_mse: 8664.2881\n",
      "Epoch 11/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 8415.4326 - mae: 66.3195 - mse: 8415.4326 - val_loss: 8980.5283 - val_mae: 69.7920 - val_mse: 8980.5283\n",
      "Epoch 12/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 7618.2998 - mae: 62.4150 - mse: 7618.2998 - val_loss: 7500.7373 - val_mae: 61.8643 - val_mse: 7500.7373\n",
      "Epoch 13/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 7162.5581 - mae: 58.9822 - mse: 7162.5581 - val_loss: 6940.6509 - val_mae: 58.2889 - val_mse: 6940.6509\n",
      "Epoch 14/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 6887.0376 - mae: 56.1690 - mse: 6887.0376 - val_loss: 6367.2842 - val_mae: 56.1765 - val_mse: 6367.2842\n",
      "Epoch 15/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 6332.2866 - mae: 53.5923 - mse: 6332.2866 - val_loss: 6005.0112 - val_mae: 53.2561 - val_mse: 6005.0112\n",
      "Epoch 16/20\n",
      "8867/8867 [==============================] - 12s 1ms/step - loss: 5796.4399 - mae: 51.1559 - mse: 5796.4399 - val_loss: 5739.5273 - val_mae: 51.4458 - val_mse: 5739.5273\n",
      "Epoch 17/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 5296.0259 - mae: 48.9356 - mse: 5296.0259 - val_loss: 5828.8086 - val_mae: 52.9114 - val_mse: 5828.8086\n",
      "Epoch 18/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 5025.4849 - mae: 46.8456 - mse: 5025.4849 - val_loss: 4795.4141 - val_mae: 45.7148 - val_mse: 4795.4141\n",
      "Epoch 19/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 4707.7808 - mae: 45.1908 - mse: 4707.7808 - val_loss: 4500.6440 - val_mae: 43.3117 - val_mse: 4500.6440\n",
      "Epoch 20/20\n",
      "8867/8867 [==============================] - 13s 1ms/step - loss: 4574.4331 - mae: 43.7224 - mse: 4574.4331 - val_loss: 4724.3442 - val_mae: 44.7288 - val_mse: 4724.3442\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/work/notebooks/../data/output/models/NN_Regression_Model_24_available_hex_big_hexa_big/assets\n",
      "R2 (Train): 0.7646481707268906\n",
      "R2 (Validation): 0.7635893769027757\n",
      "Plot training and visualization loss...\n"
     ]
    }
   ],
   "source": [
    "#Train the NN for each time resolution.\n",
    "# hex_size = [\"hexa_small\", \"hexa_big\"]\n",
    "hex_size = [\"hexa_big\", \"hexa_small\"]\n",
    "time_resolutions = [\"24_demand\", \"6_demand\", \"2_demand\", \"1_demand\"]\n",
    "\n",
    "\"\"\"\n",
    "for time in time_resolutions:\n",
    "    for size in hex_size:\n",
    "        train_NN(on=time, hex_size=size)\n",
    "        print()\n",
    "print(\"Done\")\n",
    "\"\"\"\n",
    "train_NN(on=\"24_available_hex_big\", hex_size=\"hexa_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
