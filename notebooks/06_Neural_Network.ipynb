{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "A neural network is created which can be used for training and testing on trips data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output=os.path.join(os.getcwd(), \"..\", \"data\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(os.path.join(path_output, \"Features.csv\"))\n",
    "features = features.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Params from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"data\", \"input\", \"params.csv\")).drop(\"0\", axis=1)\n",
    "_test_size = params[params[\"param\"]==\"test_size\"][\"value\"].values[0]\n",
    "_random_state = int(params[params[\"param\"]==\"random_state\"][\"value\"].values[0])\n",
    "_epochs = int(params[params[\"param\"]==\"epochs\"][\"value\"].values[0])\n",
    "_validation_size = params[params[\"param\"]==\"validation_size\"][\"value\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train_scaled, y_train, on, hex_size):\n",
    "    \"\"\"\n",
    "    Train Neural Network Model\n",
    "\n",
    "    Train and save a Neural Network model.\n",
    "    The network has the following properties:\n",
    "        - three hidden layer\n",
    "        - 50 epochs\n",
    "        - activation function is relu\n",
    "        - dimension of input and hidden layer is 36\n",
    "        - dimension of output layer is 1\n",
    "        - dropout is not used\n",
    "    Then evaluate the error metrics by another method.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled (DataFrame):   Scaled X input of train set (matrix)\n",
    "        y_train (Series):             y output to train on (vector)\n",
    "    Returns:\n",
    "        nn_regression_sets (array): true y values and predicted y values for train and validation set\n",
    "    \"\"\"\n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train, random_state=_random_state, test_size=_validation_size)\n",
    "    neural_network = keras.Sequential(\n",
    "        [layers.Dense(36, activation=\"relu\", input_shape=[X_train_scaled.shape[1]], kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dropout(0.2),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"relu\", kernel_initializer=\"random_normal\"),\n",
    "         # layers.Dense(36, activation=\"softmax\"),\n",
    "         # layers.Dense(36, activation=\"softmax\"),\n",
    "         # layers.Dropout(0.2),\n",
    "         layers.Dense(1)])\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "    neural_network.compile(loss=\"mse\",\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=[\"mae\", \"mse\"])\n",
    "    epochs = _epochs\n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    history = neural_network.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
    "    neural_network.save(os.path.join(path_output, \"models\", \"NN_Regression_Model_\"+on+\"_\"+hex_size))\n",
    "    y_prediction_train = neural_network.predict(X_train)\n",
    "    y_prediction_val = neural_network.predict(X_val)\n",
    "    plot_train_loss(history, on=on, hex_size=hex_size)\n",
    "    nn_regression_sets = [y_train, y_val, y_prediction_train, y_prediction_val]\n",
    "    return nn_regression_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss visualization by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss(history, on, hex_size):\n",
    "    \"\"\"\n",
    "    Plot the train and validation loss of Neural Network.\n",
    "\n",
    "    Args:\n",
    "        history (Object): History of loss during training of neural network\n",
    "        on (str): time resolution to train on\n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Plot training and visualization loss...\")\n",
    "    # Plotting the training and validation loss\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    fig, ax = plt.subplots(figsize=(16, 8), dpi=300)\n",
    "    ax.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    ax.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    ax.set_title(\"Training and validation loss \"+on+\"_\"+hex_size, fontsize=18)\n",
    "    ax.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    fig.savefig(os.path.join(path_output, \"NN_error_per_epoch_\"+on+\"_\"+hex_size+\".png\"))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_NN(on=\"24_sum\", hex_size=\"hexa_small\"):\n",
    "    \"\"\"\n",
    "    Split the data in train and test set by 0.3 test set. \n",
    "    Then Scale the data and do a PCA. \n",
    "    Last train the NN on the chosen time resolution\n",
    "    \n",
    "    Args:\n",
    "        on (str): time resolution to train on\n",
    "        \n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Time Resolution is\", on)\n",
    "    #print(\"Split Data with random state\", _random_state, \"and test size\", str(_test_size)+\"...\")\n",
    "    features_X = features.drop([\"24_sum\", \"6_sum\", \"2_sum\", \"1_sum\"], axis=1)\n",
    "    features_y = features[on]\n",
    "    \n",
    "    # Spatial Resolution\n",
    "    print(\"Spatial Resolution is\", hex_size)\n",
    "    if hex_size==\"hexa_small\":\n",
    "        features_X = features_X.drop(\"hexa_big\", axis=1)\n",
    "    else:\n",
    "        features_X = features_X.drop(\"hexa_small\", axis=1)\n",
    "    \n",
    "    #Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_X, features_y, random_state=_random_state, test_size=_test_size)\n",
    "\n",
    "    #print(\"Scale\", hex_size, \"Data with Standard Scaler...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"Standard_Scaler_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        standard_scaler = pickle.load(f)\n",
    "    X_train_scaled = standard_scaler.transform(X_train)\n",
    "\n",
    "    #print(\"Do PCA on\", hex_size, \"Data...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"PCA_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        pca = pickle.load(f)\n",
    "    X_train_transformed = pca.transform(X_train_scaled)\n",
    "\n",
    "    print(\"Train\", \"NN_Regression_Model_\"+on+\"_\"+hex_size, \"...\")\n",
    "    nn_regression_sets = train_neural_network(X_train_transformed, y_train.to_numpy(), on=on, hex_size=hex_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Resolution is 24_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_small\n",
      "Scale hexa_small Data with Standard Scaler...\n",
      "Do PCA on hexa_small Data...\n",
      "Train NN 24_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 74s 262us/sample - loss: 1034238.5918 - mae: 627.2525 - mse: 1034237.2500 - val_loss: 506023.5723 - val_mae: 444.5254 - val_mse: 506024.4062\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_24_sum_hexa_small\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 24_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_big\n",
      "Scale hexa_big Data with Standard Scaler...\n",
      "Do PCA on hexa_big Data...\n",
      "Train NN 24_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 62s 217us/sample - loss: 1282425.4757 - mae: 687.8499 - mse: 1282423.3750 - val_loss: 917334.3025 - val_mae: 574.0627 - val_mse: 917335.7500\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_24_sum_hexa_big\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 6_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_small\n",
      "Scale hexa_small Data with Standard Scaler...\n",
      "Do PCA on hexa_small Data...\n",
      "Train NN 6_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 71s 250us/sample - loss: 325309.9335 - mae: 311.8183 - mse: 325309.5625 - val_loss: 139327.6869 - val_mae: 216.4063 - val_mse: 139327.6406\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_6_sum_hexa_small\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 6_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_big\n",
      "Scale hexa_big Data with Standard Scaler...\n",
      "Do PCA on hexa_big Data...\n",
      "Train NN 6_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 66s 234us/sample - loss: 327054.9130 - mae: 319.8958 - mse: 327055.1562 - val_loss: 139091.3887 - val_mae: 240.3874 - val_mse: 139091.5469\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_6_sum_hexa_big\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 2_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_small\n",
      "Scale hexa_small Data with Standard Scaler...\n",
      "Do PCA on hexa_small Data...\n",
      "Train NN 2_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 56s 196us/sample - loss: 109965.6933 - mae: 179.3398 - mse: 109965.4297 - val_loss: 70571.6139 - val_mae: 133.5406 - val_mse: 70571.6719\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_2_sum_hexa_small\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 2_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_big\n",
      "Scale hexa_big Data with Standard Scaler...\n",
      "Do PCA on hexa_big Data...\n",
      "Train NN 2_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 45s 159us/sample - loss: 106953.6010 - mae: 177.5837 - mse: 106953.3125 - val_loss: 69448.4725 - val_mae: 136.3973 - val_mse: 69448.4922\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_2_sum_hexa_big\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 1_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_small\n",
      "Scale hexa_small Data with Standard Scaler...\n",
      "Do PCA on hexa_small Data...\n",
      "Train NN 1_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 64s 226us/sample - loss: 60563.5366 - mae: 130.9476 - mse: 60563.4844 - val_loss: 36953.1503 - val_mae: 108.3354 - val_mse: 36953.1602\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_1_sum_hexa_small\\assets\n",
      "Plot training and visualization loss...\n",
      "Time Resolution is 1_sum\n",
      "Split Data with random state 42 and test size 0.3...\n",
      "Spatial Resolution is hexa_big\n",
      "Scale hexa_big Data with Standard Scaler...\n",
      "Do PCA on hexa_big Data...\n",
      "Train NN 1_sum...\n",
      "Train on 283719 samples, validate on 113488 samples\n",
      "283719/283719 [==============================] - 63s 221us/sample - loss: 69502.2338 - mae: 133.7321 - mse: 69502.2266 - val_loss: 43914.5028 - val_mae: 110.6420 - val_mse: 43914.5039\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\Documents\\GitHub\\AAA_Jupyter_Explorers\\notebooks\\..\\data\\output\\models\\NN_Regression_Model_1_sum_hexa_big\\assets\n",
      "Plot training and visualization loss...\n"
     ]
    }
   ],
   "source": [
    "#Train the NN for each time resolution.\n",
    "hex_size = [\"hexa_small\", \"hexa_big\"]\n",
    "time_resolutions = [\"24_sum\", \"6_sum\", \"2_sum\", \"1_sum\"]\n",
    "for time in time_resolutions:\n",
    "    for size in hex_size:\n",
    "        train_NN(on=time, hex_size=size)\n",
    "        print()\n",
    "print(\"Done\")\n",
    "#train_NN(on=\"1_sum\", hex_size=\"hexa_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDS20",
   "language": "python",
   "name": "pds20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
