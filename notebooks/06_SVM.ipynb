{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output=os.path.join(os.getcwd(), \"..\", \"data\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(os.path.join(path_output, \"Features.csv\"))\n",
    "features = features.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Params from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(os.path.join(os.getcwd(), \"..\", \"data\", \"input\", \"params.csv\")).drop(\"0\", axis=1)\n",
    "_test_size = params[params[\"param\"]==\"test_size\"][\"value\"].values[0]\n",
    "_random_state = int(params[params[\"param\"]==\"random_state\"][\"value\"].values[0])\n",
    "_epochs = int(params[params[\"param\"]==\"epochs\"][\"value\"].values[0])\n",
    "_validation_size = params[params[\"param\"]==\"validation_size\"][\"value\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_support_vector_machine(X_train_scaled, y_train, time_res, hex_size, kernel):\n",
    "    \"\"\"\n",
    "    Train SVM\n",
    "\n",
    "    Train and save an SVM model.\n",
    "    Then evaluate the error metrics by another method.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled (DataFrame):   Scaled X input of train set (matrix)\n",
    "        y_train (Series):             y output to train on (vector)\n",
    "    Returns:\n",
    "        svm_regression_sets (array): true y values and predicted y values for train and validation set\n",
    "    \"\"\"\n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    # print(\"Splitting train test data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train, random_state=_random_state, test_size=_validation_size)\n",
    "\n",
    "    # print(\"Initializing SVR...\")\n",
    "    svr = make_pipeline(StandardScaler(), SVR(kernel=kernel, cache_size=2000))\n",
    "    # svr = SVR(kernel=kernel, max_iter=1000)\n",
    "    # print(svr)\n",
    "    # print(\"Fitting SVR...\")\n",
    "    svr.fit(X_train, y_train)\n",
    "    print(\"Score (Train):\", svr.score(X_train, y_train))\n",
    "    \n",
    "    # create a validation set which is 20% of the whole dataset. Therefore use formula to receive ca. 0.2857.\n",
    "    # print(\"Saving SVR model...\")\n",
    "    filename = \"SVR_model_\"+time_res+\"_\"+hex_size+\"_\"+kernel+\".pkl\"\n",
    "    pickle.dump(svr, open(os.path.join(os.getcwd(), \"..\", \"data\", \"output\", \"models\", filename), \"wb\"))\n",
    "    \n",
    "    # print(\"Predicting y with train data...\")\n",
    "    y_prediction_train = svr.predict(X_train)\n",
    "    # print(\"Predicting y with validation data...\")\n",
    "    y_prediction_val = svr.predict(X_val)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    # print(confusion_matrix(y_train,y_prediction_train_lin))\n",
    "    # print(classification_report(y_train,y_prediction_train_lin))\n",
    "    # print(confusion_matrix(y_val,y_prediction_val_lin))\n",
    "    # print(classification_report(y_val,y_prediction_val_lin))\n",
    "    \n",
    "    svm_regression_sets = [y_train, y_val, y_prediction_train, y_prediction_val]\n",
    "    return svm_regression_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss(svm_regression_sets, time_res, hex_size, kernel):\n",
    "    \"\"\"\n",
    "    Plot the train and validation loss of Support Vector Machine.\n",
    "\n",
    "    Args:\n",
    "        history (Object): History of loss during training of support vector machine\n",
    "        time_res (str): time resolution to train on\n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Plot difference between Train and Predicted Train...\")\n",
    "    y_train = svm_regression_sets[0]\n",
    "    y_val = svm_regression_sets[1]\n",
    "    y_prediction_train = svm_regression_sets[2]\n",
    "    y_prediction_val = svm_regression_sets[3]\n",
    "    \n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train, y_prediction_train)\n",
    "    mae_val = mean_absolute_error(y_val, y_prediction_val)\n",
    "    r2_train = r2_score(y_train, y_prediction_train)\n",
    "    r2_val = r2_score(y_val, y_prediction_val)\n",
    "    \n",
    "    print()\n",
    "    print(\"=== TRAIN ===\")\n",
    "    print(\"MAE:\", mae_train)\n",
    "    print(\"R2:\", r2_train)\n",
    "    print()\n",
    "    \n",
    "    print(\"=== VALIDATION ===\")\n",
    "    print(\"MAE:\", mae_val)\n",
    "    print(\"R2:\", r2_val)\n",
    "    print()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 8), dpi=300)\n",
    "    \n",
    "    ax1.plot(y_prediction_train, y_train, \"bo\")\n",
    "    ax1.set_title(\"Train Y vs Predicted Train Y (\"+time_res+\", \"+hex_size+\", \"+kernel+\")\", fontsize=20)\n",
    "    ax1.set_xlabel(\"Predicted Train Y\", fontsize=18)\n",
    "    ax1.set_ylabel(\"Train Y\", fontsize=18)\n",
    "    \n",
    "    ax2.plot(y_prediction_val, y_val, \"bo\")\n",
    "    ax2.set_title(\"Validation Y vs Predicted Validation Y (\"+time_res+\", \"+hex_size+\", \"+kernel+\")\", fontsize=20)\n",
    "    ax2.set_xlabel(\"Predicted Validation Y\", fontsize=18)\n",
    "    ax2.set_ylabel(\"Validation Y\", fontsize=18)\n",
    "    \n",
    "    fig.savefig(os.path.join(path_output, \"SVM_Train_Validation_Real_vs_Predicted_\"+time_res+\"_\"+hex_size+\"_\"+kernel+\".png\"))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(\"Plot training and visualization loss...\")\n",
    "    \n",
    "    # Plotting the training and validation loss\n",
    "    # loss = history.history[\"loss\"]\n",
    "    # val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    # epochs = range(1, len(loss) + 1)\n",
    "    # fig, ax = plt.subplots(figsize=(16, 8), dpi=300)\n",
    "    # ax.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    # ax.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    # ax.set_title(\"Training and validation loss \"+time_res+\"_\"+hex_size, fontsize=18)\n",
    "    # ax.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    # ax.set_ylabel(\"Loss\", fontsize=16)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # fig.savefig(os.path.join(path_output, \"SVM_error_per_epoch_\"+time_res+\"_\"+hex_size+\".png\"))\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVM(time_res=\"24_demand\", hex_size=\"hexa_big\", kernel=\"linear\"):\n",
    "    \"\"\"\n",
    "    Split the data in train and test set by 0.3 test set. \n",
    "    Then Scale the data and do a PCA. \n",
    "    Last train the SVM on the chosen time resolution\n",
    "    \n",
    "    Args:\n",
    "        time_res (str): time resolution to train on\n",
    "        \n",
    "    Returns:\n",
    "        No return\n",
    "    \"\"\"\n",
    "    print(\"Time Resolution is\", time_res)\n",
    "    #print(\"Split Data with random state\", _random_state, \"and test size\", str(_test_size)+\"...\")\n",
    "    # features_X = features.drop([\"24_sum\", \"6_sum\", \"2_sum\", \"1_sum\"], axis=1)\n",
    "    features_X = features.drop([\"24_demand\", \"24_demand_hex_big\", \"24_demand_hex_small\", \"24_agg_time\",\n",
    "                                \"6_demand\", \"6_demand_hex_big\", \"6_demand_hex_small\", \"6_agg_time\",\n",
    "                                \"2_demand\", \"2_demand_hex_big\", \"2_demand_hex_small\", \"2_agg_time\",\n",
    "                                \"1_demand\", \"1_demand_hex_big\", \"1_demand_hex_small\", \"1_agg_time\"], axis=1)\n",
    "    features_y = features[time_res]\n",
    "    \n",
    "    # Spatial Resolution\n",
    "    print(\"Spatial Resolution is\", hex_size)\n",
    "    if hex_size==\"hexa_small\":\n",
    "        features_X = features_X.drop(\"hexa_big\", axis=1)\n",
    "    else:\n",
    "        features_X = features_X.drop(\"hexa_small\", axis=1)\n",
    "    \n",
    "    # Kernel\n",
    "    print(\"Kernel is\", kernel)\n",
    "    \n",
    "    #Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_X, features_y, random_state=_random_state, test_size=_test_size)\n",
    "\n",
    "    #print(\"Scale\", hex_size, \"Data with Standard Scaler...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"Standard_Scaler_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        standard_scaler = pickle.load(f)\n",
    "    X_train_scaled = standard_scaler.transform(X_train)\n",
    "\n",
    "    print(\"Do PCA on\", hex_size, \"Data...\")\n",
    "    with open(os.path.join(path_output, \"models\", \"PCA_\"+hex_size+\".pkl\"), \"rb\") as f:\n",
    "        pca = pickle.load(f)\n",
    "    X_train_transformed = pca.transform(X_train_scaled)\n",
    "    \n",
    "    # PCA\n",
    "    # pca = PCA(n_components=10)\n",
    "    # pca.fit(X_train_scaled)\n",
    "    # pca_explained_variance = pca.explained_variance_ratio_\n",
    "    #  print(\"Var explained:\", pca_explained_variance)\n",
    "    # print(\"Sum var explained\", sum(pca_explained_variance))\n",
    "    # X_train_transformed = pca.transform(X_train_scaled)\n",
    "    # print(\"X_train_transformed:\", X_train_transformed)\n",
    "\n",
    "    print(\"Train\", \"SVM_Regression_Model_\"+time_res+\"_\"+hex_size+\"_\"+kernel, \"...\")\n",
    "    svm_regression_sets = train_support_vector_machine(X_train_transformed, y_train.to_numpy(), time_res=time_res, hex_size=hex_size, kernel=kernel)\n",
    "    plot_train_loss(svm_regression_sets, time_res=time_res, hex_size=hex_size, kernel=kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Resolution is 24_demand\n",
      "Spatial Resolution is hexa_big\n",
      "Kernel is linear\n",
      "Do PCA on hexa_big Data...\n",
      "Train SVM_Regression_Model_24_demand_hexa_big_linear ...\n",
      "Score (Train): 7.505540594776594e-05\n",
      "Plot difference between Train and Predicted Train...\n",
      "\n",
      "=== TRAIN ===\n",
      "MAE: 741.0430177767319\n",
      "R2: 7.505540594776594e-05\n",
      "\n",
      "=== VALIDATION ===\n",
      "MAE: 736.7547714736899\n",
      "R2: 0.00852102545203759\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Train the SVM for each time resolution.\n",
    "hex_size = [\"hexa_small\", \"hexa_big\"]\n",
    "time_resolutions = [\"24_demand\", \"1_demand\"]\n",
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\", \"precomputed\"]\n",
    "\n",
    "\"\"\"\n",
    "for time in time_resolutions:\n",
    "    for size in hex_size:\n",
    "        for kernel in kernels:\n",
    "            train_SVM(time_res=time, hex_size=size, kernel=kernel)\n",
    "            print()\n",
    "\"\"\"\n",
    "\n",
    "train_SVM(time_res=\"24_demand\", hex_size=\"hexa_big\", kernel=\"linear\")\n",
    "# train_SVM(time_res=\"24_demand\", hex_size=\"hexa_big\", kernel=\"sigmoid\")\n",
    "# train_SVM(time_res=\"24_demand\", hex_size=\"hexa_big\", kernel=\"precomputed\")\n",
    "# train_SVM(time_res=\"24_sum\", hex_size=\"hexa_big\", kernel=\"rbf\")\n",
    "# train_SVM(time_res=\"24_sum\", hex_size=\"hexa_big\", kernel=\"poly\")\n",
    "# train_SVM(time_res=\"1_sum\", hex_size=\"hexa_small\", kernel=\"linear\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
